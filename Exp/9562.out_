hendrixgpu15fl.unicph.domain
0
0 it number
learning rate: 1.0e-02
num_dense_layers: 1
num_dense_nodes: 10
activation: sigmoid
lamda: 0.1

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.032411 s

'compile' took 2.622523 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [2.52e-02, 7.14e-01, 5.06e-02]    [2.58e-02, 7.14e-01, 5.06e-02]    []  
2000      [2.62e-03, 6.56e-03, 6.64e-02]    [4.60e-03, 6.56e-03, 6.64e-02]    []  
4000      [4.01e-04, 1.24e-03, 5.94e-02]    [6.80e-04, 1.24e-03, 5.94e-02]    []  
6000      [4.00e-04, 1.40e-03, 5.91e-02]    [4.64e-04, 1.40e-03, 5.91e-02]    []  
8000      [4.05e-04, 1.45e-03, 5.90e-02]    [5.58e-04, 1.45e-03, 5.90e-02]    []  
10000     [3.76e-04, 1.43e-03, 5.90e-02]    [5.09e-04, 1.43e-03, 5.90e-02]    []  
12000     [4.06e-04, 1.45e-03, 5.89e-02]    [5.32e-04, 1.45e-03, 5.89e-02]    []  
Epoch 12000: early stopping

Best model at step 10000:
  train loss: 6.08e-02
  test loss: 6.09e-02
  test metric: []

Epoch 12000: saving model to Nomagnetic_5th_60/model-12000.ckpt ...

'train' took 12.293427 s

1 it number
learning rate: 6.9e-03
num_dense_layers: 8
num_dense_nodes: 19
activation: tanh
lamda: 0.07255540512037871

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.143882 s

'compile' took 24.944239 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [2.03e-01, 1.25e+00, 3.84e-02]    [1.37e-01, 1.25e+00, 3.84e-02]    []  
2000      [3.97e-08, 1.82e-03, 1.05e-01]    [6.15e-08, 1.82e-03, 1.05e-01]    []  
4000      [5.01e-03, 1.45e-02, 7.48e-02]    [4.57e-03, 1.45e-02, 7.48e-02]    []  
6000      [2.83e-08, 8.81e-03, 9.83e-02]    [1.55e-09, 8.81e-03, 9.83e-02]    []  
Epoch 6000: early stopping

Best model at step 4000:
  train loss: 9.44e-02
  test loss: 9.39e-02
  test metric: []

Epoch 6000: saving model to Nomagnetic_5th_61/model-6000.ckpt ...

'train' took 93.094753 s

2 it number
learning rate: 3.6e-04
num_dense_layers: 3
num_dense_nodes: 25
activation: sigmoid
lamda: 0.013065854800735549

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.031687 s

'compile' took 10.590482 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.10e-06, 1.24e+00, 6.63e-03]    [1.07e-06, 1.24e+00, 6.63e-03]    []  
2000      [3.76e-05, 2.14e-04, 1.95e-02]    [3.33e-05, 2.14e-04, 1.95e-02]    []  
4000      [6.52e-05, 2.64e-04, 1.94e-02]    [5.53e-05, 2.64e-04, 1.94e-02]    []  
6000      [6.71e-05, 2.66e-04, 1.94e-02]    [5.39e-05, 2.66e-04, 1.94e-02]    []  
8000      [7.00e-05, 2.64e-04, 1.94e-02]    [4.90e-05, 2.64e-04, 1.94e-02]    []  
10000     [7.92e-05, 7.02e-04, 1.62e-02]    [2.13e-04, 7.02e-04, 1.62e-02]    []  
12000     [7.75e-05, 2.55e-04, 8.52e-03]    [2.00e-04, 2.55e-04, 8.52e-03]    []  
14000     [5.16e-05, 2.00e-04, 8.12e-03]    [1.78e-04, 2.00e-04, 8.12e-03]    []  
16000     [4.18e-05, 1.45e-04, 8.09e-03]    [1.16e-04, 1.45e-04, 8.09e-03]    []  
18000     [5.31e-05, 1.04e-04, 8.10e-03]    [8.55e-05, 1.04e-04, 8.10e-03]    []  
20000     [2.31e-05, 8.64e-05, 8.10e-03]    [6.53e-05, 8.64e-05, 8.10e-03]    []  

Best model at step 20000:
  train loss: 8.21e-03
  test loss: 8.25e-03
  test metric: []

Epoch 20000: saving model to Nomagnetic_5th_62/model-20000.ckpt ...

'train' took 66.413078 s

3 it number
learning rate: 1.1e-04
num_dense_layers: 5
num_dense_nodes: 11
activation: tanh
lamda: 0.012538683368444242

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.047195 s

'compile' took 16.245392 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [4.76e+02, 6.78e-01, 7.16e-03]    [3.50e+02, 6.78e-01, 7.16e-03]    []  
2000      [9.08e-02, 1.17e-01, 9.85e-03]    [7.66e-02, 1.17e-01, 9.85e-03]    []  
4000      [4.15e-02, 9.69e-03, 1.50e-02]    [3.72e-02, 9.69e-03, 1.50e-02]    []  
6000      [1.40e-02, 1.02e-03, 1.76e-02]    [1.55e-02, 1.02e-03, 1.76e-02]    []  
8000      [4.10e-03, 3.10e-04, 1.83e-02]    [4.80e-03, 3.10e-04, 1.83e-02]    []  
10000     [1.25e-03, 3.87e-04, 1.84e-02]    [1.57e-03, 3.87e-04, 1.84e-02]    []  
12000     [4.14e-04, 3.41e-04, 1.85e-02]    [5.42e-04, 3.41e-04, 1.85e-02]    []  
14000     [1.60e-04, 2.77e-04, 1.86e-02]    [2.32e-04, 2.77e-04, 1.86e-02]    []  
16000     [1.06e-04, 2.98e-04, 1.85e-02]    [1.37e-04, 2.98e-04, 1.85e-02]    []  
18000     [9.70e-05, 3.42e-04, 1.83e-02]    [1.68e-04, 3.42e-04, 1.83e-02]    []  
20000     [1.36e-04, 3.89e-04, 1.80e-02]    [2.47e-04, 3.89e-04, 1.80e-02]    []  

Best model at step 20000:
  train loss: 1.85e-02
  test loss: 1.86e-02
  test metric: []

Epoch 20000: saving model to Nomagnetic_5th_63/model-20000.ckpt ...

'train' took 103.298198 s

4 it number
learning rate: 1.6e-04
num_dense_layers: 5
num_dense_nodes: 2
activation: sin
lamda: 0.017635770689809285

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.046911 s

'compile' took 17.558083 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [2.54e-03, 1.10e+00, 8.79e-03]    [2.09e-03, 1.10e+00, 8.79e-03]    []  
2000      [3.01e-04, 7.23e-03, 2.40e-02]    [6.35e-05, 7.23e-03, 2.40e-02]    []  
4000      [1.21e-04, 3.13e-04, 2.63e-02]    [2.42e-05, 3.13e-04, 2.63e-02]    []  
6000      [8.32e-05, 3.12e-04, 2.63e-02]    [1.67e-05, 3.12e-04, 2.63e-02]    []  
8000      [1.16e-04, 3.41e-04, 2.62e-02]    [2.81e-05, 3.41e-04, 2.62e-02]    []  
10000     [1.31e-04, 4.46e-04, 2.59e-02]    [5.81e-05, 4.46e-04, 2.59e-02]    []  
12000     [1.28e-04, 4.84e-04, 2.59e-02]    [8.26e-05, 4.84e-04, 2.59e-02]    []  
14000     [1.33e-04, 5.06e-04, 2.58e-02]    [7.98e-05, 5.06e-04, 2.58e-02]    []  
16000     [1.36e-04, 5.27e-04, 2.57e-02]    [8.06e-05, 5.27e-04, 2.57e-02]    []  
18000     [1.89e-04, 5.74e-04, 2.52e-02]    [4.49e-05, 5.74e-04, 2.52e-02]    []  
20000     [2.00e-04, 8.55e-04, 2.45e-02]    [4.41e-05, 8.55e-04, 2.45e-02]    []  

Best model at step 20000:
  train loss: 2.55e-02
  test loss: 2.54e-02
  test metric: []

Epoch 20000: saving model to Nomagnetic_5th_64/model-20000.ckpt ...

'train' took 109.158287 s

5 it number
learning rate: 5.4e-02
num_dense_layers: 9
num_dense_nodes: 30
activation: sigmoid
lamda: 0.024761808027681763

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.075978 s

'compile' took 37.900674 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [5.36e-13, 1.84e+00, 1.51e-02]    [5.11e-13, 1.84e+00, 1.51e-02]    []  
2000      [1.20e-20, 6.07e-04, 3.68e-02]    [1.16e-20, 6.07e-04, 3.68e-02]    []  
4000      [2.18e-20, 6.07e-04, 3.68e-02]    [2.00e-20, 6.07e-04, 3.68e-02]    []  
Epoch 4000: early stopping

Best model at step 2000:
  train loss: 3.75e-02
  test loss: 3.75e-02
  test metric: []

Epoch 4000: saving model to Nomagnetic_5th_65/model-4000.ckpt ...

'train' took 135.999679 s

6 it number
learning rate: 4.7e-03
num_dense_layers: 6
num_dense_nodes: 24
activation: sin
lamda: 0.027247850801547492

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.063118 s

'compile' took 20.687084 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.48e-01, 6.45e-01, 1.47e-02]    [9.54e-02, 6.46e-01, 1.47e-02]    []  
2000      [1.04e-04, 6.54e-04, 4.01e-02]    [1.15e-04, 6.50e-04, 4.01e-02]    []  
4000      [6.89e-04, 1.55e-03, 3.61e-02]    [6.66e-04, 1.54e-03, 3.61e-02]    []  
6000      [3.68e-04, 1.49e-03, 3.78e-02]    [3.39e-04, 1.50e-03, 3.78e-02]    []  
Epoch 6000: early stopping

Best model at step 4000:
  train loss: 3.84e-02
  test loss: 3.83e-02
  test metric: []

Epoch 6000: saving model to Nomagnetic_5th_66/model-6000.ckpt ...

'train' took 101.768192 s

7 it number
learning rate: 1.3e-02
num_dense_layers: 10
num_dense_nodes: 2
activation: sin
lamda: 0.02901192881449209

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.084810 s

'compile' took 45.819790 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [4.07e+05, 1.16e+00, 1.87e-02]    [5.00e+05, 1.16e+00, 1.87e-02]    []  
2000      [1.42e-01, 1.18e-03, 4.34e-02]    [8.69e-02, 1.18e-03, 4.34e-02]    []  
4000      [6.27e-02, 9.97e-04, 4.32e-02]    [3.84e-02, 9.97e-04, 4.32e-02]    []  
6000      [1.89e-02, 8.78e-04, 4.31e-02]    [1.10e-02, 8.78e-04, 4.31e-02]    []  
8000      [4.63e-03, 8.48e-04, 4.30e-02]    [2.13e-03, 8.48e-04, 4.30e-02]    []  
10000     [1.71e-03, 8.28e-04, 4.30e-02]    [5.67e-04, 8.28e-04, 4.30e-02]    []  
12000     [4.61e-04, 8.27e-04, 4.30e-02]    [1.20e-04, 8.27e-04, 4.30e-02]    []  
14000     [7.14e-05, 8.27e-04, 4.30e-02]    [2.81e-05, 8.27e-04, 4.30e-02]    []  
16000     [2.97e-05, 8.29e-04, 4.29e-02]    [1.09e-05, 8.29e-04, 4.29e-02]    []  
18000     [8.83e-06, 8.14e-04, 4.30e-02]    [3.46e-06, 8.14e-04, 4.30e-02]    []  
20000     [5.46e-06, 8.27e-04, 4.29e-02]    [4.44e-06, 8.27e-04, 4.29e-02]    []  

Best model at step 20000:
  train loss: 4.38e-02
  test loss: 4.38e-02
  test metric: []

Epoch 20000: saving model to Nomagnetic_5th_67/model-20000.ckpt ...

'train' took 249.168301 s

8 it number
learning rate: 2.5e-03
num_dense_layers: 4
num_dense_nodes: 25
activation: sin
lamda: 0.011153893091407734

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.040673 s

'compile' took 14.279131 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.79e-01, 6.49e-01, 6.13e-03]    [1.74e-01, 6.49e-01, 6.13e-03]    []  
2000      [3.03e-04, 2.09e-04, 7.27e-03]    [2.97e-04, 2.09e-04, 7.27e-03]    []  
4000      [3.63e-04, 2.15e-04, 6.89e-03]    [2.73e-04, 2.15e-04, 6.89e-03]    []  
6000      [1.89e-05, 7.16e-05, 6.90e-03]    [3.70e-05, 7.16e-05, 6.90e-03]    []  
8000      [6.99e-05, 9.00e-05, 6.90e-03]    [4.46e-05, 9.00e-05, 6.90e-03]    []  
Epoch 8000: early stopping

Best model at step 6000:
  train loss: 6.99e-03
  test loss: 7.01e-03
  test metric: []

Epoch 8000: saving model to Nomagnetic_5th_68/model-8000.ckpt ...

'train' took 104.237003 s

9 it number
learning rate: 4.8e-01
num_dense_layers: 7
num_dense_nodes: 28
activation: tanh
lamda: 0.09447169863034377

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.062007 s

'compile' took 22.952742 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.62e-02, 8.83e-01, 4.72e-02]    [1.54e-02, 8.83e-01, 4.72e-02]    []  
2000      [nan, nan, nan]                   [nan, nan, nan]                   []  

Best model at step 0:
  train loss: 9.47e-01
  test loss: 9.46e-01
  test metric: []

Epoch 2000: saving model to Nomagnetic_5th_69/model-2000.ckpt ...

'train' took 123.127688 s

10 it number
learning rate: 2.9e-03
num_dense_layers: 3
num_dense_nodes: 13
activation: tanh
lamda: 0.03978604897061095

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.033277 s

'compile' took 21.862257 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.22e+01, 7.82e-01, 2.38e-02]    [5.92e+00, 7.82e-01, 2.38e-02]    []  
2000      [1.48e-03, 2.58e-03, 4.99e-02]    [2.12e-03, 2.58e-03, 4.99e-02]    []  
4000      [9.44e-04, 1.38e-03, 2.38e-02]    [1.16e-03, 1.38e-03, 2.38e-02]    []  
6000      [3.29e-04, 5.21e-04, 2.39e-02]    [6.43e-04, 5.21e-04, 2.39e-02]    []  
8000      [2.30e-04, 4.89e-04, 2.39e-02]    [4.18e-04, 4.89e-04, 2.39e-02]    []  
10000     [4.75e-04, 5.32e-04, 2.39e-02]    [4.88e-04, 5.32e-04, 2.39e-02]    []  
Epoch 10000: early stopping

Best model at step 8000:
  train loss: 2.46e-02
  test loss: 2.48e-02
  test metric: []

Epoch 10000: saving model to Nomagnetic_5th_610/model-10000.ckpt ...

'train' took 106.177612 s

11 it number
learning rate: 3.2e-02
num_dense_layers: 1
num_dense_nodes: 30
activation: sigmoid
lamda: 0.1

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.019087 s

'compile' took 5.075334 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [2.40e-03, 1.55e+00, 5.28e-02]    [2.18e-03, 1.55e+00, 5.28e-02]    []  
2000      [1.98e-04, 1.63e-03, 5.91e-02]    [2.47e-04, 1.63e-03, 5.91e-02]    []  
4000      [4.88e-04, 1.83e-03, 5.85e-02]    [6.41e-04, 1.83e-03, 5.85e-02]    []  
6000      [8.64e-04, 1.91e-03, 5.83e-02]    [8.62e-04, 1.91e-03, 5.83e-02]    []  
Epoch 6000: early stopping

Best model at step 4000:
  train loss: 6.08e-02
  test loss: 6.09e-02
  test metric: []

Epoch 6000: saving model to Nomagnetic_5th_611/model-6000.ckpt ...

'train' took 82.778096 s

12 it number
learning rate: 9.0e-04
num_dense_layers: 10
num_dense_nodes: 1
activation: sigmoid
lamda: 0.1

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.087500 s

'compile' took 40.352136 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.62e-19, 1.18e+00, 5.03e-02]    [1.74e-19, 1.18e+00, 5.03e-02]    []  
2000      [4.18e-17, 8.59e-03, 1.36e-01]    [4.48e-17, 8.59e-03, 1.36e-01]    []  
4000      [7.49e-17, 8.59e-03, 1.36e-01]    [8.10e-17, 8.59e-03, 1.36e-01]    []  
Epoch 4000: early stopping

Best model at step 2000:
  train loss: 1.44e-01
  test loss: 1.44e-01
  test metric: []

Epoch 4000: saving model to Nomagnetic_5th_612/model-4000.ckpt ...

'train' took 176.462022 s

13 it number
learning rate: 4.2e-02
num_dense_layers: 10
num_dense_nodes: 1
activation: sigmoid
lamda: 0.01

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.086093 s

'compile' took 55.350561 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [4.56e-19, 1.69e+00, 5.77e-03]    [4.34e-19, 1.69e+00, 5.77e-03]    []  
2000      [2.85e-24, 1.02e-04, 1.52e-02]    [2.74e-24, 1.02e-04, 1.52e-02]    []  
4000      [2.81e-24, 1.02e-04, 1.52e-02]    [2.71e-24, 1.02e-04, 1.52e-02]    []  
Epoch 4000: early stopping

Best model at step 2000:
  train loss: 1.53e-02
  test loss: 1.53e-02
  test metric: []

Epoch 4000: saving model to Nomagnetic_5th_613/model-4000.ckpt ...

'train' took 193.179083 s

14 it number
learning rate: 6.0e-03
num_dense_layers: 2
num_dense_nodes: 19
activation: sigmoid
lamda: 0.05099111561396138

Warning: 200 points required, but 210 points sampled.
Compiling model...
Building feed-forward neural network...
'build' took 0.033552 s

'compile' took 10.433588 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [1.25e-03, 8.49e-02, 5.05e-02]    [1.25e-03, 8.49e-02, 5.05e-02]    []  
2000      [8.36e-04, 1.50e-03, 3.02e-02]    [1.29e-03, 1.50e-03, 3.02e-02]    []  
4000      [1.63e-04, 7.17e-04, 3.04e-02]    [3.97e-04, 7.17e-04, 3.04e-02]    []  
6000      [2.37e-04, 7.73e-04, 3.03e-02]    [2.79e-04, 7.73e-04, 3.03e-02]    []  
Epoch 6000: early stopping

Best model at step 4000:
  train loss: 3.13e-02
  test loss: 3.15e-02
  test metric: []

Epoch 6000: saving model to Nomagnetic_5th_614/model-6000.ckpt ...

'train' took 121.772888 s

[0.002469218272788538, 4, 25, 'sin', 0.011153893091407734]
